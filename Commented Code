#importing the various libraries needed in the program
from bs4 import BeautifulSoup 
from selenium import webdriver
import folium
import urllib.request
import time
import base64
import os
#function which will sometimes be needed, as folium apparently does not read all characters
def clean_letters(list_):
    ch1=['ä','ü','Ö','Ü','ö','ß']
    ch2=['a','u','O','U','O','ss']
    change_letter=dict(zip(ch1,ch2))
    for n,i in enumerate(list_):
        for num,letter in enumerate(i):
            if letter in ch1:
                let=change_letter[letter]
                list_name= list(list_[n])
                list_name[num]=let
                list_[n]=''.join(list_name)
                
cit=input('Choose City: ') 
driver=webdriver.Chrome('/Users/lodovicobenvenuti/Documents/chromedriver') #creating a browser instance
driver.get('https://ita.archinform.net/ort/')#connecting to the website to scrape from: Archinform. Using a bot because of cookies
time.sleep(1)
button=driver.find_elements_by_xpath('/html/body/div[2]/div/div/a[1]')[0]#finding the 'accept cookies' button
button.click()
time.sleep(0.5)
button=driver.find_elements_by_xpath('//*[@id="xsearch"]')[0]#finding the search bar
button.click()
time.sleep(0.5)
button.send_keys(cit)#inserting our city in the search bar

time.sleep(1)
button=driver.find_elements_by_css_selector('#ui-id-1 > li:nth-child(1)')[0]#pressing on the first result of our search
button.click()
time.sleep(1.5)
l=(str(driver.current_url)).split('/')#getting the url specifics for our city
id_=l[4].split('.')
id_città=id_[0]
driver.get('https://www.google.com/maps/')#getting google maps for our city's coordinates
time.sleep(4)#waiting 4 seconds to manually accept cookies. Unfortunately I am unable to make the program accept it
driver.get('https://www.google.com/maps/')
button=driver.find_elements_by_xpath('//*[@id="searchboxinput"]')[0]#find the search bar
button.click()
time.sleep(0.5)
button.send_keys('Città di '+cit)#inserting our city in the search bar-italian used because I am italian
time.sleep(0.5)
button=driver.find_elements_by_xpath('//*[@id="searchbox-searchbutton"]')[0]
button.click()
time.sleep(3)
l='@'.split(str(driver.current_url))#finding the coordinates, which contained in the url
s=(str(driver.current_url)).split('@')
_coord=s[1]
_coord=_coord.split(',')
coord_città=[_coord[0],_coord[1]]#storing the coordinates

def pagine(città,pnr):#function to count the pages of buildings in the website for the specific city
    driver.get('https://ita.archinform.net/ort/'+città+'.htm'+pnr)
    time.sleep(1)
    tab=driver.find_elements_by_css_selector('#MenueLSTPChild > div > div > div > p:nth-child(1)')[0]
    a=tab.get_attribute('innerHTML')
    soup= BeautifulSoup(a)
    l=soup.text
    ll=l.split('[')
    l=ll[1].split(' ')
    return(int(l[0])//50)
p=pagine(id_città,'')

projects=[]
def getprojects(città,pnr):
    driver.get('https://ita.archinform.net/ort/'+città+'.htm'+pnr)
    time.sleep(1)
    i=driver.find_elements_by_class_name('ulProj')[0]#scraping the project name and info from the website
    a=i.get_attribute('innerHTML')
    soup= BeautifulSoup(a)
    for i in soup.find_all(class_='projekte'):
        projects.append(i.text)#storing the info in projects
i=0
while i <= p:#loop to go through the different pages of the website
    print(i)
    if i == 0:
        pnr=''
    else:
        pnr='?pnr='+str(50*i+1)
    getprojects(id_città,pnr)
    i+=1
    
project_name1=[]
project_info1=[]
#dividing project names and project information
for l,i in enumerate(projects):
    if (l+2)%2==0:
        project_info1.append(i)
    else:
        project_name1.append(i)
for n,i in enumerate(project_name1):
    if '/' in i:
        i=' '.join(i.split('/'))
        project_name1[n]=i
mydict=dict(zip(project_name1,project_info1))
result = {}
#eliminating duplicates both from names and infos
for key,value in mydict.items():
    if key not in result.keys():
        result[key] = value
project_name=[]
project_info=[]
for i in result.keys():
    project_name.append(i)
for i in result.values():
    project_info.append(i)
#eliminating extra info from project info
for n,i in enumerate(project_info):
    a=i.split('\n')
    project_info[n]=' '.join(a[-3:])
    
def find_point(i):#function to find the coordinates of a specific place
    driver.get('https://www.google.com/maps/')#google maps home
    time.sleep(0.5)
    button=driver.find_elements_by_xpath('//*[@id="searchboxinput"]')[0]#searchbox
    button.click()
    time.sleep(0.5)
    button.send_keys(i)#insert building to find
    time.sleep(0.5)
    button=driver.find_elements_by_xpath('//*[@id="searchbox-searchbutton"]')[0]#search
    button.click()
    time.sleep(3)
    l='@'.split(str(driver.current_url))#get coords in url
    s=(str(driver.current_url)).split('@')
    temp_coord=s[1]
    temp_coord=temp_coord.split(',')
    a=driver.page_source
    soup=BeautifulSoup(a)
    hero=soup.find_all(class_='section-hero-header-image')#find if the search has given a specific result-hero-.
    if len(hero)==0:#if not, invalid coords
        temp_coord=[0,0]
        l=i#the name of the building is invariated
    else:
        name=driver.find_elements_by_class_name('section-hero-header-title-description')[0]#find name of the building
        a=name.get_attribute('innerHTML')
        soup= BeautifulSoup(a)
        soup=soup.find('span')
        l=soup.text#save google's name
    
    return (temp_coord,l)#return google name and coord
coordinates=[]
google_names=[]

driver=webdriver.Chrome('/Users/lodovicobenvenuti/Documents/chromedriver')
driver.get('https://www.google.com/maps/')
time.sleep(4)#in order to manually accept cookies
start=(len(coordinates))
for i in project_name[start:]:#in order not to loose the progress made in retrieving coordinates when it bugs out.
    temp_coord,l=find_point(i)#call coordinates scraping function
    if float(coord_città[0])-0.4<float(temp_coord[0])<float(coord_città[0])+0.4:
        if float(coord_città[1])-0.4<float(temp_coord[1])<float(coord_città[1])+0.4:#if coord in the city append info in coordinates and google names
            coord=[]
            for r in range(2):
                coord.append(temp_coord[r])
            coordinates.append([i,coord])
            google_names.append(l) 
        else:#if not try again searching with the name of the city at the end, in order to avoid ambiguities
            temp_coord,l=find_point(str(i+' '+cit))
            coord=[]
            for r in range(2):
                coord.append(temp_coord[r])
            coordinates.append([i,coord])
            google_names.append(l)
    else:
        temp_coord,l=find_point(str(i+' '+cit))
        coord=[]
        for r in range(2):
            coord.append(temp_coord[r])
        coordinates.append([i,coord])
        google_names.append(l)
#separating latitude from longitutde   
valid_xcoords=[]
for i in coordinates:
    valid_xcoords.append(i[1][0])
valid_ycoords=[]
for i in coordinates:
    valid_ycoords.append(i[1][1])
#formatting google names
for n,i in enumerate(google_names):
    a=i.split(' ')
    r=0
    while r < len(a):
        if a[r]=='':
            a.remove(a[r])
            r=r
        else:
            r+=1
    google_names[n]=' '.join(a)
for n,i in enumerate(google_names):
    if '/' in i:
        i=' '.join(i.split('/'))
        google_names[n]=i
for n,i in enumerate(project_info):#joining google names to project info
    r=google_names[n]+', '+i
    project_info[n]=r  
clean_letters(project_info)

src_=[]
#retrieving images for all buildings
for n,i in enumerate(google_names):
    if (n+35)%35==0:
        driver=webdriver.Chrome('/Users/lodovicobenvenuti/Documents/chromedriver')
    i=str(i+' '+cit)
    search='+'.join(i.split(' '))
    driver.get('http://www.google.com/search?q='+search+'&tbm=isch')
    a=driver.page_source
    soup=BeautifulSoup(a)
    img=soup.find(class_='Q4LuWd')
    if img!= None:
        src=img['src']
    src_.append(src)
    if '/' in i:
        i=' '.join(i.split('/'))
    urllib.request.urlretrieve(src,i+'.jpg') 
arch_map = folium.Map(location=coord_città, zoom_start=11)
#creating map with folium with tooltips which are the information of the place and markers which are the images

for lat, lng,name,label in zip(valid_xcoords, valid_ycoords, google_names, project_info):
    if lat != 0:
        html = '<img src="data:image/jpeg;base64,{}">'.format
        encoded = base64.b64encode(open(name+' '+cit+'.jpg', 'rb').read()).decode()
        iframe = folium.IFrame(html(encoded), width=300, height=150)
        popup = folium.Popup(iframe)
        icon=folium.IFrame('<i class="fas fa-archway"></i>')
        folium.Marker([lat, lng],tooltip=label,popup=popup,icon=folium.Icon(color='orange',icon='university',prefix='fa')).add_to(arch_map)

arch_map.save('Mappa di '+cit+'.html') 

#eliminating images from directory
import os

for i in project_name:
    if os.path.exists(i+' '+cit+'.jpg'):
        os.remove(i+' '+cit+'.jpg')


    
